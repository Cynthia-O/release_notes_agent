{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2824a5d-d80e-4998-a614-7cecaac92ed1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Tutorial notebook from: https://docs.databricks.com/aws/en/generative-ai/tutorials/agent-framework-notebook#notebook\n",
    "\n",
    "- Runs fine on Serverless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dd7f1f7-3271-428a-be06-00aad375cfc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Lock down some important packages\n",
    "%pip install -U -qqqq mlflow langchain langgraph==0.3.4 databricks-langchain pydantic databricks-agents unitycatalog-langchain[databricks] \n",
    "\n",
    "#anyio==3.7.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc2ab057-1e77-45f2-a3df-9b9d08e6e077",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7ee6072-bec1-4d7f-ba9f-9f44e50db4b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks_langchain import ChatDatabricks\n",
    "import mlflow\n",
    "mlflow.langchain.autolog()\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7748d638-6c76-45f8-88c0-633dedd63c5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "LLM_ENDPOINT = \"llama-instruct-3-3-70b\"\n",
    "llm = ChatDatabricks(endpoint=LLM_ENDPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ae96b6f-7490-4832-8d0c-129b14ca2ac7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "databricks_docs_url = \"https://raw.githubusercontent.com/databricks/genai-cookbook/refs/heads/main/quick_start_demo/chunked_databricks_docs_filtered.jsonl\"\n",
    "parsed_docs_df = pd.read_json(databricks_docs_url, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07cd60fd-318c-4ab5-b4f4-291f8350abe0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks_langchain.uc_ai import (\n",
    "    DatabricksFunctionClient,\n",
    "    UCFunctionToolkit,\n",
    "    set_uc_function_client,\n",
    ")\n",
    "\n",
    "uc_client = DatabricksFunctionClient()\n",
    "set_uc_function_client(uc_client)\n",
    "\n",
    "def tfidf_keywords(text: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Extracts keywords from the provided text using TF-IDF.\n",
    "\n",
    "    Args:\n",
    "        text (string): Input text.\n",
    "    Returns:\n",
    "        list[str]: List of extracted keywords in ascending order of importance.\n",
    "    \"\"\"\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "    def extract_keywords(text, top_n=5):\n",
    "        \"\"\"Extracts top keywords from input text using trained TF-IDF vectorizer\"\"\"\n",
    "        keyword_vectorizer = TfidfVectorizer(\n",
    "            stop_words=\"english\"\n",
    "        )  # New vectorizer for query\n",
    "        query_tfidf = keyword_vectorizer.fit_transform([text])  # Fit on query only\n",
    "        scores = query_tfidf.toarray()[0]\n",
    "        indices = scores.argsort()[-top_n:][::-1]  # Get top N keywords\n",
    "        return [\n",
    "            keyword_vectorizer.get_feature_names_out()[i]\n",
    "            for i in indices\n",
    "            if scores[i] > 0\n",
    "        ]\n",
    "\n",
    "    return extract_keywords(text)\n",
    "\n",
    "# TODO fill in your catalog and schema name\n",
    "catalog = \"main\"\n",
    "schema = \"default\"\n",
    "\n",
    "assert (catalog and schema)\n",
    "\n",
    "# Create the function within the Unity Catalog catalog and schema specified\n",
    "function_info = uc_client.create_python_function(\n",
    "    func=tfidf_keywords,\n",
    "    catalog=catalog,\n",
    "    schema=schema,\n",
    "    replace=True,  # Set to True to overwrite if the function already exists\n",
    ")\n",
    "\n",
    "uc_tool_names = [f\"{catalog}.{schema}.tfidf_keywords\"]\n",
    "uc_toolkit = UCFunctionToolkit(function_names=uc_tool_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e73d518-3d23-4011-a9df-795189486af3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(uc_tool_names, uc_toolkit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee0b9207-4bb9-4b25-a9a3-148754b2d644",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(uc_toolkit.tools[0])\n",
    "uc_toolkit.tools[0].invoke({\"text\": \"The quick brown fox jumped over the lazy brown dog.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bb247d7-fe16-44fe-ad3f-9936be4fc765",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "import mlflow\n",
    "from langchain_core.tools import tool\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "documents = parsed_docs_df\n",
    "doc_vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "tfidf_matrix = doc_vectorizer.fit_transform(documents[\"content\"])\n",
    "\n",
    "@tool\n",
    "@mlflow.trace(name=\"LittleIndex\", span_type=mlflow.entities.SpanType.RETRIEVER)\n",
    "def find_relevant_documents(query: str, top_n: int = 5) -> list[dict[str, Any]]:\n",
    "    \"\"\"gets relevant documents for the query\"\"\"\n",
    "    query_tfidf = doc_vectorizer.transform([query])\n",
    "    similarities = (tfidf_matrix @ query_tfidf.T).toarray().flatten()\n",
    "    ranked_docs = sorted(enumerate(similarities), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    result = []\n",
    "    for idx, score in ranked_docs[:top_n]:\n",
    "        row = documents.iloc[idx]\n",
    "        content = row[\"content\"]\n",
    "        doc_entry = {\n",
    "            \"page_content\": content,\n",
    "            \"metadata\": {\n",
    "                \"doc_uri\": row[\"doc_uri\"],\n",
    "                \"score\": score,\n",
    "            },\n",
    "        }\n",
    "        result.append(doc_entry)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5730d647-a525-46ed-85c6-4293e388923c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import Optional, Sequence, Union\n",
    "\n",
    "from langchain_core.language_models import LanguageModelLike\n",
    "from langchain_core.runnables import RunnableConfig, RunnableLambda\n",
    "from langchain_core.tools import BaseTool\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.graph.graph import CompiledGraph\n",
    "from langgraph.prebuilt.tool_node import ToolNode\n",
    "from mlflow.langchain.chat_agent_langgraph import ChatAgentState, ChatAgentToolNode\n",
    "\n",
    "def create_tool_calling_agent(\n",
    "    model: LanguageModelLike,\n",
    "    tools: Union[ToolNode, Sequence[BaseTool]],\n",
    "    agent_prompt: Optional[str] = None,\n",
    ") -> CompiledGraph:\n",
    "    model = model.bind_tools(tools)\n",
    "\n",
    "    def routing_logic(state: ChatAgentState):\n",
    "        last_message = state[\"messages\"][-1]\n",
    "        if last_message.get(\"tool_calls\"):\n",
    "            return \"continue\"\n",
    "        else:\n",
    "            return \"end\"\n",
    "\n",
    "    if agent_prompt:\n",
    "        system_message = {\"role\": \"system\", \"content\": agent_prompt}\n",
    "        preprocessor = RunnableLambda(\n",
    "            lambda state: [system_message] + state[\"messages\"]\n",
    "        )\n",
    "    else:\n",
    "        preprocessor = RunnableLambda(lambda state: state[\"messages\"])\n",
    "    model_runnable = preprocessor | model\n",
    "\n",
    "    def call_model(\n",
    "        state: ChatAgentState,\n",
    "        config: RunnableConfig,\n",
    "    ):\n",
    "        response = model_runnable.invoke(state, config)\n",
    "\n",
    "        return {\"messages\": [response]}\n",
    "    \n",
    "    workflow = StateGraph(ChatAgentState)\n",
    "\n",
    "    workflow.add_node(\"agent\", RunnableLambda(call_model))\n",
    "    workflow.add_node(\"tools\", ChatAgentToolNode(tools))\n",
    "\n",
    "    workflow.set_entry_point(\"agent\")\n",
    "    workflow.add_conditional_edges(\n",
    "        \"agent\",\n",
    "        routing_logic,\n",
    "        {\n",
    "            \"continue\": \"tools\",\n",
    "            \"end\": END,\n",
    "        },\n",
    "    )\n",
    "    workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "    return workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2dc147af-841d-4038-b234-22b0ead05a09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "agent = create_tool_calling_agent(llm, tools=[*uc_toolkit.tools, find_relevant_documents])\n",
    "agent.invoke({\"messages\": [{\"role\": \"user\", \"content\":\"What are the keywords for the sentence: 'the quick brown fox jumped over the lazy brown dog'?\"}]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd89cb90-8122-4e41-b4c2-e7124c760793",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.pyfunc import ChatAgent\n",
    "from mlflow.types.agent import (\n",
    "    ChatAgentChunk,\n",
    "    ChatAgentMessage,\n",
    "    ChatAgentResponse,\n",
    "    ChatContext,\n",
    ")\n",
    "from typing import Any, Optional\n",
    "\n",
    "class DocsAgent(ChatAgent):\n",
    "  def __init__(self, agent):\n",
    "    self.agent = agent\n",
    "\n",
    "  def predict(\n",
    "      self,\n",
    "      messages: list[ChatAgentMessage],\n",
    "      context: Optional[ChatContext] = None,\n",
    "      custom_inputs: Optional[dict[str, Any]] = None,\n",
    "  ) -> ChatAgentResponse:\n",
    "      # ChatAgent has a built-in helper method to help convert framework-specific messages, like langchain BaseMessage to a python dictionary\n",
    "      request = {\"messages\": self._convert_messages_to_dict(messages)}\n",
    "\n",
    "      output = agent.invoke(request)\n",
    "      # Here 'output' is already a ChatAgentResponse, but to make the ChatAgent signature explicit for this demonstration we are returning a new instance\n",
    "      return ChatAgentResponse(**output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d1f4caf-b061-4a4e-8a4d-eae77b8509b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "AGENT = DocsAgent(agent=agent)\n",
    "AGENT.predict({\"messages\": [{\"role\": \"user\", \"content\": \"What is DLT in Databricks?\"}]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7584d705-4b01-45f5-a407-cb89d746934b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.models import ModelConfig\n",
    "\n",
    "baseline_config = {\n",
    "   \"endpoint_name\": \"databricks-meta-llama-3-3-70b-instruct\",\n",
    "   \"temperature\": 0.01,\n",
    "   \"max_tokens\": 1000,\n",
    "   \"system_prompt\": \"\"\"You are a helpful assistant that answers questions about Databricks. Questions unrelated to Databricks are irrelevant.\n",
    "\n",
    "    You answer questions using a set of tools. If needed, you ask the user follow-up questions to clarify their request.\n",
    "    \"\"\",\n",
    "   \"tool_list\": [f\"{catalog}.{schema}.*\"],\n",
    "}\n",
    "\n",
    "class DocsAgent(ChatAgent):\n",
    "    def __init__(self):\n",
    "        self.config = ModelConfig(development_config=baseline_config)\n",
    "        self.agent = self._build_agent_from_config()\n",
    "\n",
    "    def _build_agent_from_config(self):\n",
    "        temperature = self.config.get(\"temperature\")\n",
    "        max_tokens = self.config.get(\"max_tokens\")\n",
    "        system_prompt = self.config.get(\"system_prompt\")\n",
    "        llm_endpoint_name = self.config.get(\"endpoint_name\")\n",
    "        tool_list = self.config.get(\"tool_list\")\n",
    "\n",
    "        llm = ChatDatabricks(endpoint=llm_endpoint_name, temperature=temperature, max_tokens=max_tokens)\n",
    "        toolkit = UCFunctionToolkit(function_names=tool_list)\n",
    "        agent = create_tool_calling_agent(llm, tools=[*toolkit.tools, find_relevant_documents], agent_prompt=system_prompt)\n",
    "\n",
    "        return agent\n",
    "    \n",
    "    def predict(\n",
    "        self,\n",
    "        messages: list[ChatAgentMessage],\n",
    "        context: Optional[ChatContext] = None,\n",
    "        custom_inputs: Optional[dict[str, Any]] = None,\n",
    "    ) -> ChatAgentResponse:\n",
    "        # ChatAgent has a built-in helper method to help convert framework-specific messages, like langchain BaseMessage to a python dictionary\n",
    "        request = {\"messages\": self._convert_messages_to_dict(messages)}\n",
    "\n",
    "        output = self.agent.invoke(request)\n",
    "        # Here 'output' is already a ChatAgentResponse, but to make the ChatAgent signature explicit for this demonstration we are returning a new instance\n",
    "        return ChatAgentResponse(**output)\n",
    "    \n",
    "agent = DocsAgent()\n",
    "agent.predict({\"messages\": [{\"role\": \"user\", \"content\": \"What is DLT\"}]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6ed0f2b-4c7a-4283-8c51-16576624152f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%writefile getting_started_agent.py\n",
    "from typing import Any, Optional, Sequence, Union\n",
    "\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "from databricks_langchain import ChatDatabricks\n",
    "from databricks_langchain.uc_ai import (\n",
    "    DatabricksFunctionClient,\n",
    "    UCFunctionToolkit,\n",
    "    set_uc_function_client,\n",
    ")\n",
    "from langchain_core.language_models import LanguageModelLike\n",
    "from langchain_core.runnables import RunnableConfig, RunnableLambda\n",
    "from langchain_core.tools import BaseTool, tool\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.graph.graph import CompiledGraph\n",
    "from langgraph.prebuilt.tool_node import ToolNode\n",
    "from mlflow.langchain.chat_agent_langgraph import ChatAgentState, ChatAgentToolNode\n",
    "from mlflow.models import ModelConfig\n",
    "from mlflow.pyfunc import ChatAgent\n",
    "from mlflow.types.agent import (\n",
    "    ChatAgentMessage,\n",
    "    ChatAgentResponse,\n",
    "    ChatContext,\n",
    ")\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "databricks_docs_url = \"https://raw.githubusercontent.com/databricks/genai-cookbook/refs/heads/main/quick_start_demo/chunked_databricks_docs_filtered.jsonl\"\n",
    "parsed_docs_df = pd.read_json(databricks_docs_url, lines=True)\n",
    "\n",
    "documents = parsed_docs_df\n",
    "doc_vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "tfidf_matrix = doc_vectorizer.fit_transform(documents[\"content\"])\n",
    "\n",
    "@tool\n",
    "@mlflow.trace(name=\"LittleIndex\", span_type=mlflow.entities.SpanType.RETRIEVER)\n",
    "def find_relevant_documents(query: str, top_n: int = 5) -> list[dict[str, Any]]:\n",
    "    \"\"\"gets relevant documents for the query\"\"\"\n",
    "    query_tfidf = doc_vectorizer.transform([query])\n",
    "    similarities = (tfidf_matrix @ query_tfidf.T).toarray().flatten()\n",
    "    ranked_docs = sorted(enumerate(similarities), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    result = []\n",
    "    for idx, score in ranked_docs[:top_n]:\n",
    "        row = documents.iloc[idx]\n",
    "        content = row[\"content\"]\n",
    "        doc_entry = {\n",
    "            \"page_content\": content,\n",
    "            \"metadata\": {\n",
    "                \"doc_uri\": row[\"doc_uri\"],\n",
    "                \"score\": score,\n",
    "            },\n",
    "        }\n",
    "        result.append(doc_entry)\n",
    "    return result\n",
    "\n",
    "def create_tool_calling_agent(\n",
    "    model: LanguageModelLike,\n",
    "    tools: Union[ToolNode, Sequence[BaseTool]],\n",
    "    agent_prompt: Optional[str] = None,\n",
    ") -> CompiledGraph:\n",
    "    model = model.bind_tools(tools)\n",
    "\n",
    "    def routing_logic(state: ChatAgentState):\n",
    "        last_message = state[\"messages\"][-1]\n",
    "        if last_message.get(\"tool_calls\"):\n",
    "            return \"continue\"\n",
    "        else:\n",
    "            return \"end\"\n",
    "\n",
    "    if agent_prompt:\n",
    "        system_message = {\"role\": \"system\", \"content\": agent_prompt}\n",
    "        preprocessor = RunnableLambda(\n",
    "            lambda state: [system_message] + state[\"messages\"]\n",
    "        )\n",
    "    else:\n",
    "        preprocessor = RunnableLambda(lambda state: state[\"messages\"])\n",
    "    model_runnable = preprocessor | model\n",
    "\n",
    "    def call_model(\n",
    "        state: ChatAgentState,\n",
    "        config: RunnableConfig,\n",
    "    ):\n",
    "                response = model_runnable.invoke(state, config)\n",
    "\n",
    "        return {\"messages\": [response]}\n",
    "\n",
    "    workflow = StateGraph(ChatAgentState)\n",
    "\n",
    "    workflow.add_node(\"agent\", RunnableLambda(call_model))\n",
    "    workflow.add_node(\"tools\", ChatAgentToolNode(tools))\n",
    "\n",
    "    workflow.set_entry_point(\"agent\")\n",
    "    workflow.add_conditional_edges(\n",
    "        \"agent\",\n",
    "        routing_logic,\n",
    "        {\n",
    "            \"continue\": \"tools\",\n",
    "            \"end\": END,\n",
    "        },\n",
    "    )\n",
    "    workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "    return workflow.compile()\n",
    "\n",
    "class DocsAgent(ChatAgent):\n",
    "    def __init__(self, config, tools):\n",
    "        # Load config\n",
    "        # When this agent is deployed to Model Serving, the configuration loaded here is replaced with the config passed to mlflow.pyfunc.log_model(model_config=...)\n",
    "        self.config = ModelConfig(development_config=config)\n",
    "        self.tools = tools\n",
    "        self.agent = self._build_agent_from_config()\n",
    "\n",
    "    def _build_agent_from_config(self):\n",
    "        llm = ChatDatabricks(\n",
    "            endpoint=self.config.get(\"endpoint_name\"),\n",
    "            temperature=self.config.get(\"temperature\"),\n",
    "            max_tokens=self.config.get(\"max_tokens\"),\n",
    "        )\n",
    "        agent = create_tool_calling_agent(\n",
    "            llm,\n",
    "            tools=self.tools,\n",
    "            agent_prompt=self.config.get(\"system_prompt\"),\n",
    "        )\n",
    "        return agent\n",
    "    \n",
    "    def predict(\n",
    "        self,\n",
    "        messages: list[ChatAgentMessage],\n",
    "        context: Optional[ChatContext] = None,\n",
    "        custom_inputs: Optional[dict[str, Any]] = None,\n",
    "    ) -> ChatAgentResponse:\n",
    "        # ChatAgent has a built-in helper method to help convert framework-specific messages, like langchain BaseMessage to a python dictionary\n",
    "        request = {\"messages\": self._convert_messages_to_dict(messages)}\n",
    "\n",
    "        output = self.agent.invoke(request)\n",
    "        # Here 'output' is already a ChatAgentResponse, but to make the ChatAgent signature explicit for this demonstration we are returning a new instance\n",
    "        return ChatAgentResponse(**output)\n",
    "    \n",
    "catalog = \"main\"\n",
    "schema = \"default\"\n",
    "\n",
    "LLM_ENDPOINT = LLM_ENDPOINT\n",
    "\n",
    "baseline_config = {\n",
    "    \"endpoint_name\": LLM_ENDPOINT,\n",
    "    \"temperature\": 0.01,\n",
    "    \"max_tokens\": 1000,\n",
    "    \"system_prompt\": \"\"\"You are a helpful assistant that answers questions about Databricks. Questions unrelated to Databricks are irrelevant.\n",
    "\n",
    "    You answer questions using a set of tools. If needed, you ask the user follow-up questions to clarify their request.\n",
    "    \"\"\",\n",
    "}\n",
    "\n",
    "tools = [find_relevant_documents]\n",
    "uc_client = DatabricksFunctionClient()\n",
    "set_uc_function_client(uc_client)\n",
    "uc_toolkit = UCFunctionToolkit(function_names=[f\"{catalog}.{schema}.*\"])\n",
    "tools.extend(uc_toolkit.tools)\n",
    "\n",
    "\n",
    "AGENT = DocsAgent(baseline_config, tools)\n",
    "mlflow.models.set_model(AGENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f3764df-2dc3-4ed3-b1cd-a8b2a0971402",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Demo_Notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
